{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7289d5c",
   "metadata": {},
   "source": [
    "# A1 Train the generative synthetic cluster model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229d95b5",
   "metadata": {},
   "source": [
    "Owner: **Tamas Norbert Varga** @vargatn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0074d7c0",
   "metadata": {},
   "source": [
    "This notebook will describe how to set up and train the cluster model\n",
    "\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "    1 Load and collate galaxy catalogs \n",
    "    \n",
    "    2 Set up cluster line-of-sight emulation script (build KDE models)\n",
    "    \n",
    "    3 INTENSIVE: Draw samples from the proposal galaxy catalog and calculate survival scores for rejection sampling.\n",
    "\n",
    "## Setup\n",
    "\n",
    "This notebook relies on the:\n",
    "\n",
    "\n",
    "    * synthetic package & dependencies\n",
    "    \n",
    "    * DC2 data files hosted separately\n",
    "    \n",
    "\n",
    "## Output\n",
    "\n",
    "    * samples drawn from the KDE models\n",
    "    * scores of the samples drawn from the KDE models, the score is the P(sample)\n",
    "\n",
    "The output files are placed in the `./data/` folder\n",
    "\n",
    "\n",
    "**the calculations in this notebook are time consuming, and are intended for an OpenMP HPC environment** (not NERSC. These cells are currently commented out.),\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ecf05eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitsio as fio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import copy\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import healpy as hp\n",
    "import matplotlib as mpl\n",
    "import subprocess as sp\n",
    "\n",
    "import scipy.interpolate as interpolate\n",
    "import pickle as pickle\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "# this package is installed from https://github.com/vargatn/skysampler/tree/lsst-dev  \n",
    "# import skysampler_lsst.emulator as emulator\n",
    "# import skysampler_lsst.utils as utils\n",
    "# from skysampler_lsst.reader import result_reader\n",
    "\n",
    "import synthetic.tools as tools\n",
    "import synthetic.emulator.emulator as emulator\n",
    "import synthetic.emulator.indexer as indexer\n",
    "import synthetic.emulator.reader as reader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcf9047",
   "metadata": {},
   "source": [
    "*** Important *** set the `nprocess` value to the amount of CPU cores you are willing or allowed to use for this notebook\n",
    "\n",
    "please consider your local server, e.g. don't run heavy calculations on public login nodes\n",
    "\n",
    "***Note, this notebook is computationally heavey to execute, consider restricitng the number of galaxies you want to metacalibrate to reduce runtime when testing***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0dd5b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "nprocess = 4\n",
    "# nprocess = 160\n",
    "\n",
    "nsamples = 400 # number of propsal samples to draw\n",
    "# nsamples = 1600000 # number of propsal samples to draw\n",
    "nrepeats = 1 # number of times the full run is repeated\n",
    "# nrepeats = 4 # number of times the full run is repeated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8404328",
   "metadata": {},
   "source": [
    "## Setting up the file structure\n",
    "\n",
    "The data files for this example calculation are pre packaged, and should be downloaded from a link provided in the [data access](DATA.md) instructions\n",
    "\n",
    "    1 dc2-alpha_concentric_sample-v01_test-03.tar.gz\n",
    "    2 dc2_cluster_sim_cutouts/cosmoDC2_v1.1.4_refpixels.h5\n",
    "    3 dc2_cluster_sim_cutouts/clust_dc2-sim-LOS_v1.h5\n",
    "    \n",
    "These should be downloaded and placed in a file structure such that\n",
    "\n",
    "    /root/\n",
    "    |----/resamples/ \n",
    "    |----/dc2-alpha_concentric_sample-v01_test-03.tar.gz\n",
    "    |----/dc2_cluster_sim_cutouts/cosmoDC2_v1.1.4_refpixels.h5\n",
    "    |----/dc2_cluster_sim_cutouts/clust_dc2-sim-LOS_v1.h5\n",
    "    \n",
    "from within the root folder, extract the .tar.gz file using the command\n",
    "\n",
    "    tar xzf dc2-alpha_concentric_sample-v01_test-03.tar.gz -C  resamples --strip-components 1    \n",
    "    \n",
    "This should yield a file structure as below\n",
    " \n",
    "     /root/\n",
    "    |----/resamples/ \n",
    "    |--------------/dc2-alpha_concentric_sample-v01_test-03_run0_1846435878_rbin0.p\n",
    "    |--------------/dc2-alpha_concentric_sample-v01_test-03_run0_1846435878_rbin0_samples.fits\n",
    "    |--------------/dc2-alpha_concentric_sample-v01_test-03_run0_1846435878_rbin0_scores.fits\n",
    "            .\n",
    "            .\n",
    "            .\n",
    "    |--------------/dc2-alpha_concentric_sample-v01_test-03_run3_664487101_rbin3.p\n",
    "    |--------------/dc2-alpha_concentric_sample-v01_test-03_run3_664487101_rbin3_samples.fits\n",
    "    |--------------/dc2-alpha_concentric_sample-v01_test-03_run3_664487101_rbin3_scores.fits            \n",
    "    |----/dc2-alpha_concentric_sample-v01_test-03.tar.gz\n",
    "    |----/dc2_cluster_sim_cutouts/cosmoDC2_v1.1.4_refpixels.h5\n",
    "    |----/dc2_cluster_sim_cutouts/clust_dc2-sim-LOS_v1.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7b35c3",
   "metadata": {},
   "source": [
    " Paths for data preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b593942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cutout paths string\n",
    "# cutout_fname_base = \"/e/ocean1/users/vargatn/LSST/DC2_1.1.4/clusters_v01/dc2_cluster_sim_cutouts/clust-{}_dc2-sim-cutout.h5\"\n",
    "# # cutout output name\n",
    "# cutout_oname = \"/e/ocean1/users/vargatn/LSST/DC2_1.1.4/clusters_v01/dc2_cluster_sim_cutouts/clust_dc2-sim-LOS_v1.h5\"\n",
    "\n",
    "# # reference pixel filename input\n",
    "# fname_refpixel = \"/e/ocean1/users/vargatn/LSST/DC2_1.1.4/clusters_v01/dc2_cluster_sim_cutouts/cosmoDC2_v1.1.4_refpixel-{}.h5\"\n",
    "# # reference pixel filename output\n",
    "# oname_refpixel = \"/e/ocean1/users/vargatn/LSST/DC2_1.1.4/clusters_v01/dc2_cluster_sim_cutouts/cosmoDC2_v1.1.4_refpixels.h5\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369aa64e",
   "metadata": {},
   "source": [
    "Path for resampling calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d6567fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths for calculations\n",
    "root_path = \"/e/ocean1/users/vargatn/LSST/DC2_1.1.4/clusters_v01/\"\n",
    "deep_data_path = root_path + \"dc2_cluster_sim_cutouts/cosmoDC2_v1.1.4_refpixels.h5\"\n",
    "wide_data_path = root_path + \"dc2_cluster_sim_cutouts/clust_dc2-sim-LOS_v1.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f5c42c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_root = \"dc2-example\" # this is what the current output files will be saved as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baba02bc",
   "metadata": {},
   "source": [
    " # Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04263fec",
   "metadata": {},
   "source": [
    "## Concatenate cluster catalog cutouts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a9b6e6",
   "metadata": {},
   "source": [
    "We are concatenating the galaxy catalog cutouts around clusters. These are initially saved in a separate file for each cluster. For further processing these are saved again in a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "93c97daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redmapper catalog paths\n",
    "# redmapper_path = \"/e/ocean1/users/vargatn/LSST/DC2_1.1.4/clusters_v01/dc2_cluster_sim_cutouts/cosmoDC2_v1.1.4_redmapper_v0.7.5_clust.h5\"\n",
    "# clusters = pd.read_hdf(redmapper_path, key=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0d401e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii = (clusters[\"richness\"] > 30) & (clusters[\"richness\"] < 60) & (clusters[\"redshift\"] > 0.3) & (clusters[\"redshift\"] < 0.35)\n",
    "# print(ii.sum())\n",
    "# clusters[ii].to_hdf(\"./data/refdata/dc2_redmapper_example_sel.h5\", key=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3a6d37",
   "metadata": {},
   "source": [
    "Note that above we restricts the redshift range to 0.3 - 0.35. This is done as the emulated clusters will be representative of the ensemble properties of the selection. A narrow redshift range minimizes the intrinsic spread in apparent photometry properties.\n",
    "\n",
    "There are 41 clusters in this selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ff0edab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = pd.read_hdf(\"./data/refdata/dc2_redmapper_example_sel.h5\", key=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "46988736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table = []\n",
    "# for i, cid in enumerate(clusters[\"cluster_id\"]):\n",
    "#     print(i, cid)\n",
    "#     fname = cutout_fname_base.format(cid)\n",
    "#     tab = pd.read_hdf(fname, key=\"data\")\n",
    "#     tab[\"cluster_id\"] = cid\n",
    "#     tab = tab[tab[\"R\"] < 16 ]\n",
    "#     table.append(tab)\n",
    "# table = pd.concat(table)\n",
    "# table.to_hdf(cutout_oname, key=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d4cef",
   "metadata": {},
   "source": [
    "## reference field data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d1fab2",
   "metadata": {},
   "source": [
    "Since we saved all galaxies from three randomly selected healpix pixels, we are now concatenating them for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "35bd5b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixels = [8786, 8791, 9937]\n",
    "# refpixel = []\n",
    "# for pix in pixels:\n",
    "#     print(pix)\n",
    "#     tmp = pd.read_hdf(fname_refpixel.format(pix), key=\"data\")\n",
    "#     refpixel.append(tmp)\n",
    "# refpixel = pd.concat(refpixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d37edd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refpixel[\"R\"] = np.sqrt(np.random.uniform(0, 16**2., size=len(refpixel)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6503d3b",
   "metadata": {},
   "source": [
    "We assign a mock uniform radial profile to reference field galaxies. In case of availability, this can be replaced by the radial profile around random points (such as redmapper randoms). That approach is bit more advanced, and captures the un-evennes and edges in the survey footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ba0e6093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refpixel.to_hdf(oname_refpixel, key=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4312a2dc",
   "metadata": {},
   "source": [
    "# Train KDE model and calculate survival scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "16708b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NREPEATS = nrepeats # number of propsal samples to draw\n",
    "NSAMPLES = nsamples # number of propsal samples to draw\n",
    "NCHUNKS = nprocess # number of CPU cores to use\n",
    "bandwidth=0.1 # Gaussian KDE bandwidth in the eigen-feature space after applying PCA\n",
    "\n",
    "# data paths, the code will create a subfolder within root_path based on tag_root\n",
    "\n",
    "# The number of galaxies goes with surface area element, to avoid modeling very un-balanced PDFs \n",
    "# a series of nested concentric segments are modeled consecutively and later stiched together\n",
    "LOGR_DRAW_RMINS = np.array([-3, -0.5, 0., 0.5])\n",
    "LOGR_DRAW_RMAXS = np.array([-0.5, 0., 0.5, 1.2])\n",
    "LOGR_CAT_RMAXS = [0., 0.5, 1.1, 1.2]\n",
    "\n",
    "# feature aliases and definitions from the deep / reference dataset for comparison with the wide dataset\n",
    "deep_c_settings = {\n",
    "    \"columns\": [\n",
    "        (\"MAG_I\", \"mag_i\"),\n",
    "        (\"COLOR_G_R\", (\"mag_g\", \"mag_r\", \"-\")),\n",
    "        (\"COLOR_R_I\", (\"mag_r\", \"mag_i\", \"-\")),\n",
    "    ],\n",
    "    \"logs\": [False, False, False, False],\n",
    "    \"limits\": [(17, 22.5), (-1, 3), (-1, 3), (-1, 3)],\n",
    "}\n",
    "\n",
    "# feature aliases and definitions for all features we want to model and inherit from the deep / reference fields\n",
    "deep_smc_settings = {\n",
    "    \"columns\": [\n",
    "        (\"GABS\", (\"ellipticity_1_true\", \"ellipticity_2_true\", \"SQSUM\")),\n",
    "        (\"SIZE\", \"size_true\"),\n",
    "        (\"MAG_I\", \"mag_i\"),\n",
    "        (\"COLOR_G_R\", (\"mag_g\", \"mag_r\", \"-\")),\n",
    "        (\"COLOR_R_I\", (\"mag_r\", \"mag_i\", \"-\")),\n",
    "        (\"COLOR_I_Z\", (\"mag_i\", \"mag_z\", \"-\")),\n",
    "        (\"STELLAR_MASS\", \"stellar_mass\"),\n",
    "        (\"HALO_MASS\", \"halo_mass\")\n",
    "    ],\n",
    "    \"logs\": [False, True, False, False, False, False, True, True],\n",
    "    \"limits\": [(0., 1.), (-1, 5), (17, 25), (-1, 3), (-1, 3), (-1, 3), (10**3, 10**13), (10**9, 10**16)],\n",
    "}\n",
    "\n",
    "# feature aliases and definitions from wide dataset\n",
    "wide_cr_settings = {\n",
    "    \"columns\": [\n",
    "        (\"MAG_I\", \"mag_i\"),\n",
    "        (\"COLOR_G_R\", (\"mag_g\", \"mag_r\", \"-\")),\n",
    "        (\"COLOR_R_I\", (\"mag_r\", \"mag_i\", \"-\")),\n",
    "        (\"LOGR\", \"R\"),\n",
    "    ],\n",
    "    \"logs\": [False, False, False, True],\n",
    "    \"limits\": [(17, 22.5), (-1, 3), (-1, 3), (1e-3, 16.), ],\n",
    "}\n",
    "\n",
    "# the radial profile around clusters from the wide dataset\n",
    "wide_r_settings = {\n",
    "    \"columns\": [\n",
    "        (\"MAG_I\", \"mag_i\"),\n",
    "        (\"LOGR\", \"R\"),\n",
    "    ],\n",
    "    \"logs\": [False, True,],\n",
    "    \"limits\": [(17, 22.5), (1e-3, 16.),],\n",
    "}\n",
    "# features to use for rejection sampling\n",
    "columns = {\n",
    "    \"cols_dc\": [\"COLOR_G_R\", \"COLOR_R_I\",],\n",
    "    \"cols_wr\": [\"LOGR\",],\n",
    "    \"cols_wcr\": [\"COLOR_G_R\", \"COLOR_R_I\", \"LOGR\",],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f65c1eb",
   "metadata": {},
   "source": [
    "The below script carries out most heavy lifting calculation\n",
    "\n",
    "1) loading the data\n",
    "\n",
    "2) constructs the features from the above dictionaries\n",
    "\n",
    "3) transforms features into their eigien-space and builds a KDE\n",
    "\n",
    "4) draws NSAMPLES proposal points from the features in  deep_smc_settings \n",
    "\n",
    "5) scores each proposal point based on the KDE models of the other features. (scores are transformed according to PCA jacobian for each feature space)\n",
    "\n",
    "6) saves samples, scores, and jacobian for each draw\n",
    "\n",
    "This section is commented out as it takes a ~few hunderd CPU hours to run and it's not optimized for NERSC job managers. Currently it was ran in a local computing resource at LMU Munich.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a38d1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started reading\n",
      "creating output folder\n",
      "/e/ocean1/users/vargatn/LSST/DC2_1.1.4/clusters_v01/dc2-example/\n",
      "NRBINS: 4\n",
      "running repeat 0 out of 1\n",
      "dc2-example_run0\n",
      "starting concentric shell resampling\n",
      "rbin 0\n",
      "/e/ocean1/users/vargatn/LSST/DC2_1.1.4/clusters_v01/dc2-example//dc2-example_run0_1909520610_rbin0\n",
      "(865, 1)\n",
      "(865,)\n",
      "(865, 3)\n",
      "(865,)\n",
      "(220873, 2)\n",
      "(220873,)\n",
      "(1513572, 8)\n",
      "(1513572,)\n",
      "(1481, 3)\n",
      "(1481,)\n",
      "/e/ocean1/users/vargatn/LSST/DC2_1.1.4/clusters_v01/dc2-example//dc2-example_run0_1909520610_rbin0_samples.fits\n",
      "calculating scores\n",
      "finished calculating scores\n",
      "/e/ocean1/users/vargatn/LSST/DC2_1.1.4/clusters_v01/dc2-example//dc2-example_run0_1909520610_rbin0_scores.fits\n",
      "rbin 1\n",
      "/e/ocean1/users/vargatn/LSST/DC2_1.1.4/clusters_v01/dc2-example//dc2-example_run0_1909520610_rbin1\n",
      "(8597, 1)\n",
      "(8597,)\n",
      "(8597, 3)\n",
      "(8597,)\n",
      "(220873, 2)\n",
      "(220873,)\n",
      "(1513572, 8)\n",
      "(1513572,)\n",
      "(9421, 3)\n",
      "(9421,)\n",
      "/e/ocean1/users/vargatn/LSST/DC2_1.1.4/clusters_v01/dc2-example//dc2-example_run0_1909520610_rbin1_samples.fits\n",
      "calculating scores\n",
      "finished calculating scores\n",
      "/e/ocean1/users/vargatn/LSST/DC2_1.1.4/clusters_v01/dc2-example//dc2-example_run0_1909520610_rbin1_scores.fits\n",
      "rbin 2\n",
      "/e/ocean1/users/vargatn/LSST/DC2_1.1.4/clusters_v01/dc2-example//dc2-example_run0_1909520610_rbin2\n",
      "(136435, 1)\n",
      "(136435,)\n"
     ]
    }
   ],
   "source": [
    "print(\"started reading\")\n",
    "refpixel = pd.read_hdf(deep_data_path, key=\"data\")\n",
    "table = pd.read_hdf(wide_data_path, key=\"data\")\n",
    "\n",
    "print(\"creating output folder\")\n",
    "root_path = root_path + tag_root + \"/\"\n",
    "print(root_path)\n",
    "if not os.path.isdir(root_path):\n",
    "    os.mkdir(root_path)\n",
    "\n",
    "nrbins = len(LOGR_DRAW_RMINS)\n",
    "print(\"NRBINS:\", nrbins)\n",
    "\n",
    "for nrep in np.arange(NREPEATS):\n",
    "    tag = tag_root + \"_run\" + str(nrep)\n",
    "    print(\"running repeat\", nrep, \"out of\", NREPEATS)\n",
    "    print(tag)\n",
    "\n",
    "    master_seed = np.random.randint(0, np.iinfo(np.int32).max, 1)[0]\n",
    "    rng = np.random.RandomState(seed=master_seed)\n",
    "    seeds = rng.randint(0, np.iinfo(np.int32).max, nrbins * 5)\n",
    "\n",
    "    i = 0\n",
    "    print(\"starting concentric shell resampling\")\n",
    "    for i in np.arange(nrbins):\n",
    "        print(\"rbin\", i)\n",
    "        outname = root_path + \"/\" + tag + \"_{:1d}\".format(master_seed) + \"_rbin{:d}\".format(i)\n",
    "        print(outname)\n",
    "\n",
    "        # loading random data\n",
    "        tmp_wide_r_settings = wide_r_settings.copy()\n",
    "        tmp_wide_r_settings[\"limits\"][-1] = (10**-3, 10**LOGR_CAT_RMAXS[i])\n",
    "        _wide_r_settings_rands = emulator.construct_deep_container(refpixel, tmp_wide_r_settings, seed=seeds[nrbins * i + 0], drop=\"MAG_I\")\n",
    "\n",
    "        tmp_wide_cr_settings = wide_cr_settings.copy()\n",
    "        tmp_wide_cr_settings[\"limits\"][-1] = (10**-3, 10**LOGR_CAT_RMAXS[i])\n",
    "        _wide_cr_settings_rands = emulator.construct_deep_container(refpixel, tmp_wide_cr_settings, seed=seeds[nrbins * i + 1], drop=\"MAG_I\")\n",
    "\n",
    "        # loading deep catalogs\n",
    "        _deep_c_settings = emulator.construct_deep_container(refpixel, deep_c_settings, seed=seeds[nrbins * i + 2], drop=\"MAG_I\")\n",
    "        _deep_smc_settings = emulator.construct_deep_container(refpixel, deep_smc_settings, seed=seeds[nrbins * i + 3])\n",
    "\n",
    "        # loading cluster data\n",
    "        tmp_wide_cr_settings = wide_cr_settings.copy()\n",
    "        tmp_wide_cr_settings[\"limits\"][-1] = (10**-3, 10**LOGR_CAT_RMAXS[i])\n",
    "        _wide_cr_settings_clust = emulator.construct_deep_container(table, tmp_wide_cr_settings, seed=seeds[nrbins * i + 4], drop=\"MAG_I\")\n",
    "\n",
    "        infodicts, samples = emulator.make_classifier_infodicts(_wide_cr_settings_clust, _wide_r_settings_rands,\n",
    "                                                                _wide_cr_settings_rands,\n",
    "                                                                _deep_c_settings, _deep_smc_settings,\n",
    "                                                                columns, nsamples=NSAMPLES, nchunks=NCHUNKS,\n",
    "                                                                bandwidth=bandwidth,\n",
    "                                                                rmin=LOGR_DRAW_RMINS[i],\n",
    "                                                                rmax=LOGR_DRAW_RMAXS[i])\n",
    "\n",
    "        fname = outname + \"_samples.fits\"\n",
    "        print(fname)\n",
    "        fio.write(fname, samples.to_records(), clobber=True)\n",
    "        master_dict = {\n",
    "            \"columns\": infodicts[0][\"columns\"],\n",
    "            \"bandwidth\": infodicts[0][\"bandwidth\"],\n",
    "            \"deep_c_settings\": deep_c_settings,\n",
    "            \"deep_smc_settings\": deep_smc_settings,\n",
    "            \"wide_r_settings\": tmp_wide_r_settings,\n",
    "            \"wide_cr_settings\": tmp_wide_cr_settings,\n",
    "            \"rmin\": infodicts[0][\"rmin\"],\n",
    "            \"rmax\": infodicts[0][\"rmin\"],\n",
    "        }\n",
    "        pickle.dump(master_dict, open(outname + \".p\", \"wb\"))\n",
    "        print(\"calculating scores\")\n",
    "        result = emulator.run_scores2(infodicts)\n",
    "        print(\"finished calculating scores\")\n",
    "        fname = outname + \"_scores.fits\"\n",
    "        print(fname)\n",
    "#         fio.write(fname, result.to_records(), clobber=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc5a3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64417efa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
