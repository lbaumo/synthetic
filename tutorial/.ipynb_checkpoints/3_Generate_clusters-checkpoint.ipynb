{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a46ac265",
   "metadata": {},
   "source": [
    "# Inspecting synthetic galaxy catalogs\n",
    "\n",
    "The purpose of this notebook is to run create a large sample of synthetic cluster members.  Later on these synthetic cluster members will be used to populate synthetic galaxy clusters\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "This notebook relies on two prerequisites:\n",
    "\n",
    "    * Data files from the previous calculation step. That is the output of the rejection sampling\n",
    "    * synthetic software package. The host package of this tutorial\n",
    "\n",
    "\n",
    "## Output\n",
    "\n",
    "The outuput of this claculation is a very large set of synthetic galaxies sampled from the\n",
    "    \n",
    "    * galaxy clusters member model\n",
    "    * reference line of sight model\n",
    "    \n",
    "## Contact\n",
    "\n",
    "In case of questions, contact me at t.varga@physik.lmu.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "593cdeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import fitsio as fio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import healpy as hp\n",
    "import copy\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn.decomposition as decomp\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "import subprocess as sp\n",
    "import scipy.interpolate as interpolate\n",
    "import pickle as pickle\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import synthetic.tools as tools\n",
    "import synthetic.emulator.emulator as emulator\n",
    "import synthetic.emulator.indexer as indexer\n",
    "import synthetic.emulator.reader as reader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0c554d",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ba516b",
   "metadata": {},
   "source": [
    "## Setting up the file structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a60327",
   "metadata": {},
   "source": [
    "The data files for this example calculation are pre packaged, and should be downloaded from a link provided upon request.\n",
    "\n",
    "    1 dc2-alpha_concentric_sample-v01_test-03.tar.gz\n",
    "    2 dc2_cluster_sim_cutouts/cosmoDC2_v1.1.4_refpixels.h5\n",
    "    3 dc2_cluster_sim_cutouts/clust_dc2-sim-LOS_v1.h5\n",
    "    \n",
    "These should be downloaded and placed in a file structure such that\n",
    "\n",
    "    /root/\n",
    "    |----/resamples/ \n",
    "    |----/dc2-alpha_concentric_sample-v01_test-03.tar.gz\n",
    "    |----/dc2_cluster_sim_cutouts/cosmoDC2_v1.1.4_refpixels.h5\n",
    "    |----/dc2_cluster_sim_cutouts/clust_dc2-sim-LOS_v1.h5\n",
    "    \n",
    "from within the root folder, extract the .tar.gz file using the command\n",
    "\n",
    "    tar xzf dc2-alpha_concentric_sample-v01_test-03.tar.gz -C  resamples --strip-components 1    \n",
    "    \n",
    "This should yield a file structure as below\n",
    " \n",
    "     /root/\n",
    "    |----/resamples/ \n",
    "    |--------------/dc2-alpha_concentric_sample-v01_test-03_run0_1846435878_rbin0.p\n",
    "    |--------------/dc2-alpha_concentric_sample-v01_test-03_run0_1846435878_rbin0_samples.fits\n",
    "    |--------------/dc2-alpha_concentric_sample-v01_test-03_run0_1846435878_rbin0_scores.fits\n",
    "            .\n",
    "            .\n",
    "            .\n",
    "    |--------------/dc2-alpha_concentric_sample-v01_test-03_run3_664487101_rbin3.p\n",
    "    |--------------/dc2-alpha_concentric_sample-v01_test-03_run3_664487101_rbin3_samples.fits\n",
    "    |--------------/dc2-alpha_concentric_sample-v01_test-03_run3_664487101_rbin3_scores.fits            \n",
    "    |----/dc2-alpha_concentric_sample-v01_test-03.tar.gz\n",
    "    |----/dc2_cluster_sim_cutouts/cosmoDC2_v1.1.4_refpixels.h5\n",
    "    |----/dc2_cluster_sim_cutouts/clust_dc2-sim-LOS_v1.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1854b983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU SHOULD CHANGE THE ROOT PATH TO YOUR OWN PATH\n",
    "root_path = \"/e/ocean1/users/vargatn/LSST/DC2_1.1.4/clusters_v01/\"\n",
    "deep_data_path = root_path + \"dc2_cluster_sim_cutouts/cosmoDC2_v1.1.4_refpixels.h5\"\n",
    "wide_data_path = root_path + \"dc2_cluster_sim_cutouts/clust_dc2-sim-LOS_v1.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6af62d",
   "metadata": {},
   "source": [
    "## Configuration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c554edc",
   "metadata": {},
   "source": [
    "There are two primary inputs for this calculation: A low dimensional anchor distribution, and a high dimensional augmentation distribution. The distributions are estimated from datasets via KDE, and the dimensionality refers to the number of columns (features) in each dataset.\n",
    "\n",
    "Both datasets are extracted from a catalog through a series of pre-processing steps. These steps are:\n",
    "\n",
    "    (1) a shorthand name for each feature e.g. MAG_I, which can replace a more complicated name often encountered in astronomical databases e.g. mag_auto_deblend_mcal_v2_iband. This gives the user the task to identify columns in different catalogs as conceptually the same quantity, such as calibrated i-band magnitudes in different observations of the same *kind* of objects should be the same quantity, even if the catalog columns have different names\n",
    "    \n",
    "    (2) an instruction for transforming the catalog column. E.g. colors are defined as g - r color is specified as (\"mag_g\", \"mag_r\", \"-\"). A constant transformation is also allowed, e.g. z+1 is specified as (\"z\", 1, \"+\")\n",
    "    \n",
    "    (3) logarithmic scaling for each column, which is applied after the previous transformation. The boolean list for this should be specified in the same order as the column names in the first step\n",
    "    \n",
    "    (4) limits for clipping each column to a range. The list of tuples of limits for this should be specified in the same order as the column names in the first step.\n",
    "\n",
    "\n",
    "Since the current example is defined for a simulated sky survey, both distributions are derived from the same galaxy catalog, so their selection in this notebook are identical. **NOTE This is only true for this notebook, in order to compare predictions to the ground truth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bacfe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_smc_settings = {\n",
    "    \"columns\": [\n",
    "        (\"GABS\", (\"ellipticity_1_true\", \"ellipticity_2_true\", \"SQSUM\")),\n",
    "        (\"SIZE\", \"size_true\"),\n",
    "        (\"MAG_I\", \"mag_i\"),\n",
    "        (\"COLOR_G_R\", (\"mag_g\", \"mag_r\", \"-\")),\n",
    "        (\"COLOR_R_I\", (\"mag_r\", \"mag_i\", \"-\")),\n",
    "        (\"COLOR_I_Z\", (\"mag_i\", \"mag_z\", \"-\")),\n",
    "        (\"STELLAR_MASS\", \"stellar_mass\"),\n",
    "        (\"HALO_MASS\", \"halo_mass\")        \n",
    "    ],\n",
    "    \"logs\": [False, True, False, False, False, False, True, True],\n",
    "    \"limits\": [(0., 1.), (-1, 5), (17, 25), (-1, 3), (-1, 3), (-1, 3), (10**3, 10**13), (10**9, 10**16)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "614dd382",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_cr_settings = {\n",
    "    \"columns\": [\n",
    "        (\"GABS\", (\"ellipticity_1_true\", \"ellipticity_2_true\", \"SQSUM\")),\n",
    "        (\"SIZE\", \"size_true\"),\n",
    "        (\"MAG_I\", \"mag_i\"),\n",
    "        (\"COLOR_G_R\", (\"mag_g\", \"mag_r\", \"-\")),\n",
    "        (\"COLOR_R_I\", (\"mag_r\", \"mag_i\", \"-\")),\n",
    "        (\"COLOR_I_Z\", (\"mag_i\", \"mag_z\", \"-\")),\n",
    "        (\"STELLAR_MASS\", \"stellar_mass\"),\n",
    "        (\"HALO_MASS\", \"halo_mass\"),\n",
    "        (\"LOGR\", \"R\"),        \n",
    "    ],\n",
    "    \"logs\": [False, True, False, False, False, False, True, True, True],\n",
    "    \"limits\": [(0., 1.), (-1, 5), (17, 25), (-1, 3), (-1, 3), (-1, 3), (10**3, 10**13), (10**9, 10**16), (1e-3, 16)],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1164458d",
   "metadata": {},
   "source": [
    "Only a few columns are actually used to connect the anchor and augmentation distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5997b6",
   "metadata": {},
   "source": [
    "# Loading data from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815d848",
   "metadata": {},
   "source": [
    "## Loading the regular data tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf0558",
   "metadata": {},
   "source": [
    "First load the two primary obsered datasets, since this is a simulated scenarion both are comprehensive and contain all relevant quantities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4b3f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "refpixel = pd.read_hdf(deep_data_path, key=\"data\")\n",
    "table = pd.read_hdf(wide_data_path, key=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da83cd3f",
   "metadata": {},
   "source": [
    "Then extract the desired columns and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d43520c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1513572, 9)\n",
      "(1513572,)\n",
      "(1513572, 8)\n",
      "(1513572,)\n",
      "(1009658, 9)\n",
      "(1009658,)\n"
     ]
    }
   ],
   "source": [
    "# loading random point reference data, in this case this is identical to the agumentation data\n",
    "tmp_wide_cr_settings = wide_cr_settings.copy()\n",
    "_wide_cr_settings_rands = emulator.construct_deep_container(refpixel, tmp_wide_cr_settings)\n",
    "\n",
    "# loading augmentation catalogs, in this case this is a randomly chosen helpix pixel in the simulation\n",
    "_deep_smc_settings = emulator.construct_deep_container(refpixel, deep_smc_settings)\n",
    "\n",
    "# loading anchor data, in this case this is the cluster in the simulation\n",
    "tmp_wide_cr_settings = wide_cr_settings.copy()\n",
    "_wide_cr_settings_clust = emulator.construct_deep_container(table, tmp_wide_cr_settings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e0cd7a",
   "metadata": {},
   "source": [
    "## Loading the results of rejection sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef6fe4c",
   "metadata": {},
   "source": [
    "Reading from file the results of the rejection sampling\n",
    "\n",
    "These consist of the random samples which were evaluated, and of the P(x) values, that is the scores corresponding to those sampled data points.\n",
    "\n",
    "Due numerical optimization, these files are saved separately. The different files are read separately, and then concatenated into a large data frame for each nested radial shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c90682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "scores = []\n",
    "for rbin in np.arange(4):\n",
    "    print(rbin)\n",
    "    expr = root_path + \"resamples/dc2-alpha_concentric_sample-v01_test-03/dc2-alpha_concentric_sample-v01_test-03_run0*_rbin\" + str(rbin) + \"*samples.fits\" \n",
    "\n",
    "    fnames_samples = np.sort(glob.glob(expr))\n",
    "    fnames_scores = []\n",
    "    for fname in fnames_samples:\n",
    "        fnames_scores.append(fname.replace(\"samples.fits\", \"scores.fits\"))\n",
    "\n",
    "    samples_sep = []\n",
    "    scores_sep = []\n",
    "    for i, fname in enumerate(fnames_samples):\n",
    "#         print(fname)\n",
    "        samples_sep.append(fio.read(fname))\n",
    "        scores_sep.append(fio.read(fnames_scores[i]))\n",
    "        \n",
    "    samples.append(np.hstack(samples_sep))\n",
    "    scores.append(np.hstack(scores_sep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e706f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_lims = (17, 22.5)\n",
    "r_lims_all = [(-1.5, -0.5), (-0.5, 0.), (0, 0.5), (0.5, 1.0)]\n",
    "redges = [-1.5, -0.5, 0., 0.5, 1.0]\n",
    "rareas = np.array([np.pi*((10**redges[i+1])**2. - (10**redges[i])**2.) for i in np.arange(len(redges)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4bd83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "magcol = \"mag_i\"\n",
    "ii = ((table[magcol] > mag_lims[0]) & (table[magcol] < mag_lims[1]))\n",
    "\n",
    "clust_los_nums = np.histogram(np.log10(table[ii][\"R\"]), bins=redges)[0] / 41 # / nc\n",
    "\n",
    "ii = ((refpixel[magcol] > mag_lims[0]) & (refpixel[magcol] < mag_lims[1]))\n",
    "surfdens = len(refpixel[ii]) / hp.nside2pixarea(32, degrees=True) / 3600 / 3\n",
    "rands_los_nums = surfdens * rareas\n",
    "# rands_los_nums = np.histogram(np.log10(refpixel[ii][\"R\"]), bins=redges)[0] / rareas#* ratio# / nr\n",
    "nratios = clust_los_nums / rands_los_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c1e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ifields2 = []\n",
    "iclusts2 = []\n",
    "i2ds2 = []\n",
    "for rbin in np.arange(4):\n",
    "    print(rbin)\n",
    "    _ifield, _iclust, _i2d = reader.result_reader2(samples[rbin], scores[rbin], nratio=nratios[rbin], m_factor=100, seed=rbin)\n",
    "    ifields2.append(_ifield)\n",
    "    iclusts2.append(_iclust)\n",
    "    i2ds2.append(_i2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4995f3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "csamples = []\n",
    "for rbin in np.arange(4):\n",
    "    print(rbin)\n",
    "    tab = pd.DataFrame.from_records(samples[rbin][iclusts2[rbin]].byteswap().newbyteorder())\n",
    "    tab.drop('index', axis=1, inplace=True)\n",
    "    kde = emulator.KDEContainer(tab)\n",
    "    kde.standardize_data()\n",
    "    kde.construct_kde(0.1)\n",
    "    _csample = kde.random_draw(4e6)\n",
    "    csamples.append(_csample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fc37a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "opath = \"/e/ocean1/users/vargatn/LSST/SYNTHETIC/csamples_nov_dev_01.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeb1f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.save(opath, csamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9289ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctab = _wide_cr_settings_clust[\"container\"].data\n",
    "rtab = _wide_cr_settings_rands[\"container\"].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b37ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "opath = \"/e/ocean1/users/vargatn/LSST/SYNTHETIC/csamples_nov_dev_01_ctab.h5\"\n",
    "ctab.to_hdf(opath, key=\"data\")\n",
    "opath = \"/e/ocean1/users/vargatn/LSST/SYNTHETIC/csamples_nov_dev_01_rtab.h5\"\n",
    "rtab.to_hdf(opath, key=\"data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
